# hebrew-lemmatizer

<div dir="rtl">

## אמל"ק

במסגרת תכנית ה NLP הלאומית פיתחו Korra.ai ומפא"ת פלאגין לחיפוש בעברית לElasticsearch-. הפלאגין מבוסס על מודל למטיזציה של Dictabert, יחד עם אלגוריתם להורדת Stopwords. הפלאגין והמודל מפורסמים לציבור לשימוש חופשי.

## רקע

בנית פלטפורמה לחיפוש בעברית וערבית היא חלק מתכנית ה NLP הלאומית בעברית וערבית.

<https://nnlp-il.mafat.ai/#Our-Github>

&nbsp;במסגרת זו, מובילה מפא"ת תכנית הכוללת מספר רב של פרויקטים.  מטרת התכנית היא לפתח אבני יסוד תשתיתיות לעיבוד שפות טבעיות ולהעמיד אותן לרשות קהילת העוסקים בתחום. העבודה המאסיבית על משאבי-יסוד תביא לפיתוח מואץ של יישומי קצה.  אבני היסוד כוללות בניית מאגרי מידע מתוייגים, מודל שפה ענק (LLM), ופיתוח יכולות ושירותים מתקדמים בעברית וערבית בקוד פתוח (open source) כשירות למדענים, ארגונים ממשלתיים וחברות מסחריות. פרויקט החיפוש בעברית הוא נדבך חשוב מתוך כלל המשאבים, ומהווה תרומה קריטית למשאבים הציבוריים שניתנים לכול ללא עלות.

המערכת הנפוצה ביותר לחיפוש היא Elasticsearch ("אלסטיק"), וארגונים רבים בישראל ובעולם, משתמשים בה לחיפוש תוך- וחוץ- ארגוני . אלסטיק נוצרה כפלטפורמת קוד פתוח ובבסיסה עומד אלגוריתם 25BM אשר מממש נוסחה סטטיסטית לאחזור מסמכים לפי תדירות הופעת מילות מפתח בהם. עם מהפכת ה-AI חברת אלאסטיק הוסיפה גם אפשרות לחיפוש וקטורי והוספת מודלים נוירונלים, כדי להמשיך ולשמר את מעמד הפלטפורמה כפתרון הסטנדרטי לחיפוש.

האם לאחר מהפכת ה-AI יש צורך בחיפוש מבוסס מילות מפתח? התשובה היא חיובית. ראשית, מילים רבות הינן ספציפיות לתחום ידע מסוים או לארגון מסוים, ולפיכך אין מודל אשר מכיר אותן. זו בעיה הקיימת ביחוד בחיפוש תוך ארגוני. שנית, מתוך נסיון, המודלים הנוירונליים בעברית עדיין אינם בוגרים דיים ויש עדיין צורך בתמיכה של מילות מפתח.

מהי למטיזציה ומדוע יש בה צורך? למטיזציה דומה להוצאת שורש למילה. בפעלים, הלמטיזציה הופכת אותם לגוף שלישי יחיד, למשל הולכים – הולך. בשמות עצם הלמטיזציה מעבירה ליחיד: נשים – אשה. בעולם החיפוש משתמשים בה כדי לנרמל את שאלת החיפוש ואת המסמכים נשואיו לאותו בסיס. למשל, אם השאלה היא "באילו בניינים הותקנו צינורות ברזל", למטיזציה תאפשר לאחזר מסמכים המכילים משפטים כגון "בבנין 17 מותקן צינור ברזל".

מהן הבעיות שיש לפתור בבואנו לבצע למטיזציה? שפות כמו עברית וערבית הן שפות עשירות מבחינה מורפולוגית, מכך נובעות שתי בעיות עיקריות. ראשית, בכל מילה חבויות מספר מילים. למשל המילה "שבבתיהם" במשפט "האזרחים שבבתיהם מותקן צינור ברזל" תתורגם בשפה האנגלית ל-that inside their homes - הווה אומר, ארבע מילים חבויות בתוך מילה אחת! בנוסף לכך, מאחר ורוב המסמכים בישראל הינם ללא ניקוד, הבנת המילה תלויה לחלוטין בהקשר. למשל, המילה "שמן" יכולה להיות, became fat their name, oil, their oil, fat, from which – שש אפשרויות שונות.

במשך השנים פותחו מספר פיתרונות לבעיה – החל מ[שימוש במילון](http://hspell.ivrix.org.il/) ועד למודל [למידת מכונה](https://github.com/OnlpLab/yap). לאחרונה, הוציאה עמותת דיקטה מודל נוירונלי, המבוסס על Dictabert, מודל הבסיס שפתחו. בפרויקט זה לקחנו מודל זה ובנינו לו ממשק שמאפשר עבודה שלו עם אלסטיק. ניתן לקרוא על המודל [כאן](https://dicta.org.il/developers).

הורדת ה-stopwords מתבצעת כדי לנטרל השפעה של מילים נפוצות מדי שאין להן משמעות בחיפוש נוירונלי, כגון "אז" "אם" "לא" וכו'. לצורך כאן בצענו ניתוח סטטיסטי של מספר קורפוסים שונים, ובחרנו מהם את המילים הנפוצות ביותר.

### ארכיטקטורה
![Architecture](/architecture.png)


הלמטיזציה מתבצעת בשני זמנים: בזמן אינדוקס ובזמן חיפוש. בשני זמנים אלו התהליך זהה. אלסטיק מעביר את הטקסט אל הפלאגין, והפלאגין מריץ את מודל DictaBERT ישירות בתוך ה‑JVM באמצעות ONNX Runtime (ללא שירות חיצוני וללא Docker). הורדת ה‑stopwords כלולה באותו פלאגין (אין צורך בפלאגין נפרד), והטוקנים הנותרים מועברים לאלסטיק להמשך שרשרת הביצוע.

כאמור, למודל יש מספר גרסאות ואנו ערכנו בדיקות דיוק ובדיקות מהירות בתצורות שונות, למודל ה-TINY, המאפשר הרצה על שרת רגיל ללא צורך בGPU. קיבלנו בהן את הערכים [הבאים]([url](https://drive.google.com/file/d/16DBh0EFsnIkTPvLKvZEOGhAyMuT2Tatj/view)):
[
טבלת הבדיקות
](https://drive.google.com/file/d/16DBh0EFsnIkTPvLKvZEOGhAyMuT2Tatj/view)
אורך ה-batch המקסימלי הינו 512 תווים.

דף הדגמה ונסיון נמצא [כאן]([url](http://heb.korra.ai:8080/)): http://heb.korra.ai:8080/
<div dir="rtl">
## הוראות התקנה ושימוש (Embedded ONNX, ללא Docker):<div dir="rtl">
1. בצע git clone לרפוזיטורי זה<div dir="rtl">
2. יש להריץ git lfs pull 
<div dir="rtl">
3. ייצוא והמרה של המודל ל‑ONNX עם כימות INT8:
<div dir="rtl">
<pre>
cd hebrew-lemmatizer-embedded/model-export
python3 export_model.py
</pre>
<div dir="rtl">
4. בניית הפלאגין (מומלץ בלינוקס/Elastic Cloud):
<div dir="rtl">
<pre>
docker run --rm \
  -v "/path/to/repo:/workspace" \
  -w /workspace/hebrew-lemmatizer-embedded/plugin-lemmas-embedded \
  eclipse-temurin:21-jdk ./gradlew clean bundlePlugin
</pre>
<div dir="rtl">
5. התקנת הפלאגין ל‑Elasticsearch והצגת אנלייז:
<div dir="rtl">
<pre>
/path/to/elasticsearch/bin/elasticsearch-plugin remove heb-lemmas-embedded-plugin
/path/to/elasticsearch/bin/elasticsearch-plugin install file:///path/to/heb-lemmas-embedded-plugin-2.0-SNAPSHOT.zip
/path/to/elasticsearch/bin/elasticsearch
</pre>
<div dir="rtl">
6. בדיקה:
<div dir="rtl">
<pre>
curl -k -X POST "https://localhost:9200/_analyze" -H "Content-Type: application/json" \
  -u "elastic:&lt;password&gt;" \
  -d '{"tokenizer":"whitespace","filter":["heb_lemmas","heb_stopwords"],"text":"האזרחים שבבתיהם"}'
</pre>

<div dir="rtl">
## הוראות שינוי גרסה (ES 9.x):
1. עדכון הגרסאות ב‑`hebrew-lemmatizer-embedded/plugin-lemmas-embedded/build.gradle`:
   - `ext.elasticsearchVersion`
   - `ext.luceneVersion` (לפי הגרסה של ES)
   - (אופציונלי) `ext.onnxRuntimeVersion`
2. הרצת `export_model.py` מחדש ליצירת מודל ONNX INT8.
3. בנייה מחדש עם `bundlePlugin` (עדיף בלינוקס).
4. התקנה מחדש של הפלאגין והפעלה מחדש של ES.

#### הערות:
<div dir="rtl">
1. יש לוודא ש‑git lfs מותקן לפני ה‑clone.
<div dir="rtl">
2. המודל `dicta‑tiny` מבוסס CPU; הוספת מעבדים מעל 8 לרוב לא משפרת מהירות.
<div dir="rtl">
3. הפלאגין דורש entitlements (load_native_libraries, manage_threads) — זה מגיע עם ה‑zip.

   
   
